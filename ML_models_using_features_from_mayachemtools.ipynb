{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some hightlights of my suggested approach\n",
    "\n",
    "### Feature extraction\n",
    "The current set of features (with 15 features) is rather small. I suggest that we use more features. Of course increasing the number of features while having a small training set (just about 50 data points) will increase the likelihood of overfitting. When this happens we can apply regularization methods like L1, L2 or dropout to solve the overfitting problem.\n",
    "\n",
    "* The first set of features we can try is from `dc.feat.RDKitDescriptors` which generates 200 features. They describe chemical and physical properties of a molecule. I beleive that the 15 features you generated from **mayachemtools** is just a subset of the one generated from `dc.feat.RDKitDescriptors`. We can use this set of feature with both shallow and deep learning.\n",
    "\n",
    "* The next step to try is to use molecular fingerprints from `DeepChem`. I think the two most commonly used approaches are \"Extended Connectivity Fingerprint\" (or ECFP) and `GaphConv`. `GaphConv` is state of the art (https://arxiv.org/abs/1509.09292). However it is an embedding approach and we have to use it as the top layer in a neural network model. If we want to use `GaphConv` fingerprint for shallow models such as Random Forest or XGBoost, we have to train a unsuppervised embedding model first.\n",
    "\n",
    "\n",
    "### Models\n",
    "* First we should try *Linear Regression* and *Logistic Regression* as baseline models for regression and classification problems. Then we can use more complex model such as *Random Forest*, *XGBoost* and deeplearning models. From my experience, `XGBoost` works extremly well for classification problems, so it is worth trying it.\n",
    "\n",
    "\n",
    "### Training and model selection\n",
    "* Since our training set is rather small, we should not split it into three sets (train, validation and test). Instead we should split into two: train and test. Since we don't have a validation set, we can use cross-validation to optimize the hyperparameters and select the best overall model using the test set. After having the best model, we retrain using all the data before doing prediction on the ZINC dataset.\n",
    "\n",
    "* To optimize the hyperparameter, we should use the library `hyperopt` which is the most popular Bayesian Optimization library for machine learning.\n",
    "\n",
    "\n",
    "**However, I think these suggested approaches may only work if we have more training data. The Current dataset with 52 data points is too small.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some heper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollinearColumnRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold, col_regex=None, exclude_cols=None):\n",
    "        \"\"\"\n",
    "        :param threshold: float in [0, 1], if two columns have correlation greater than threshold\n",
    "                          one of them will be removed\n",
    "        :param col_regex: str, regular expression to select columns\n",
    "        \"\"\"\n",
    "        self._threshold = threshold\n",
    "        self._col_regex = col_regex\n",
    "        if exclude_cols is None:\n",
    "            self._exclude_cols = []\n",
    "        else:\n",
    "            self._exclude_cols = exclude_cols\n",
    "    \n",
    "    def _collinear_columns(self, df, threshold):\n",
    "        if self._col_regex is None:\n",
    "            df_sel = df.select_dtypes([\"number\", \"bool\"])\n",
    "        else:\n",
    "            df_sel = df.filter(regex=self._col_regex)\n",
    "            df_sel = df_sel.select_dtypes([\"number\", \"bool\"])\n",
    "        \n",
    "        df_sel = df_sel.astype(\"float32\")\n",
    "        \n",
    "        all_cols = df_sel.columns.to_list()\n",
    "        all_cols = [col for col in all_cols if col not in self._exclude_cols]\n",
    "        df_sel = df_sel[all_cols]\n",
    "        ncols = len(all_cols)\n",
    "        \n",
    "        corr_mat = df_sel.corr().abs()\n",
    "        self._corr_mat = corr_mat\n",
    "        collin_cols = []\n",
    "        for i in range(ncols-1):\n",
    "            col_i = all_cols[i]\n",
    "            if col_i in collin_cols:\n",
    "                continue\n",
    "            \n",
    "            for j in range(i + 1, ncols):\n",
    "                col_j = all_cols[j]\n",
    "                if col_j in collin_cols:\n",
    "                    continue\n",
    "                \n",
    "                corr = corr_mat.loc[col_i, col_j]\n",
    "                if corr > threshold:\n",
    "                    collin_cols.append(col_j)\n",
    "        \n",
    "        collin_cols = list(set(collin_cols))\n",
    "        return collin_cols\n",
    "    \n",
    "    \n",
    "    def fit(self, df):\n",
    "        self._collin_cols = self._collinear_columns(df, self._threshold)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        all_cols = df.columns.to_list()\n",
    "        nonexist_cols = [col for col in self._collin_cols if col not in all_cols]\n",
    "        if len(nonexist_cols) > 0:\n",
    "            print(\"WARNING: These collinear cols to be droped do not exist in df:\", nonexist_cols)\n",
    "            \n",
    "        droped_col = [col for col in self._collin_cols if col in all_cols]\n",
    "        print(\"Number of columns droped due to collinearity:\", len(droped_col))\n",
    "        return df.drop(droped_col, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(estimator, X_eval, y_eval):\n",
    "    \"\"\"\n",
    "    :param estimator: sklearn estimator that have predict_proba() method\n",
    "    :param X_eval: test features\n",
    "    :param y_eval: test target\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    proba = estimator.predict_proba(X_eval)\n",
    "    return roc_auc_score(y_eval, proba[:, 1])\n",
    "\n",
    "\n",
    "def rmse(estimator, X_eval, y_eval):\n",
    "    y_hat = estimator.predict(X_eval)\n",
    "    return np.square(mean_squared_error(y_eval, y_hat))\n",
    "\n",
    "\n",
    "def r2(estimator, X_eval, y_eval):\n",
    "    y_hat = estimator.predict(X_eval)\n",
    "    return r2_score(y_eval, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_to_int(a_dict):\n",
    "    new_dict = copy.deepcopy(a_dict)\n",
    "    for k, v in new_dict.items():\n",
    "        if np.isclose(np.round(v), v):\n",
    "            new_dict[k] = int(new_dict[k])\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def hyperopt_cl(classifier,\n",
    "                params_tuned, \n",
    "                X_train, y_train,\n",
    "                num_eval,\n",
    "                params_fixed=None,\n",
    "                rstate=None):\n",
    "    \n",
    "    time_start = time.time()\n",
    "    if params_fixed is None:\n",
    "        params_fixed = {}\n",
    "    \n",
    "    def objective(params):\n",
    "        classifier.set_params(**params_fixed, **params)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        auc = cross_val_score(classifier, X_train, y_train, cv=5, scoring=\"roc_auc\").mean()\n",
    "        return {\"loss\": -auc, \"status\": STATUS_OK}\n",
    "    \n",
    "    if rstate is not None:\n",
    "        rstate = np.random.RandomState(rstate)\n",
    "        \n",
    "    trials = Trials()\n",
    "    best_params = fmin(objective, \n",
    "                      params_tuned, \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=num_eval, \n",
    "                      trials=trials,\n",
    "                      rstate=rstate)\n",
    "    \n",
    "    best_params = whole_to_int(best_params)\n",
    "    best_model = classifier.set_params(**params_fixed, **best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_elapse = time_end - time_start\n",
    "    print(\"Time elapsed: %0.5f s\" % time_elapse)\n",
    "    return trials, best_params, best_model\n",
    "\n",
    "\n",
    "\n",
    "def hyperopt_reg(regressor,\n",
    "                 params_tuned, \n",
    "                 X_train, y_train,\n",
    "                 num_eval,\n",
    "                 params_fixed=None,\n",
    "                 rstate=None):\n",
    "    \n",
    "    time_start = time.time()\n",
    "    if params_fixed is None:\n",
    "        params_fixed = {}\n",
    "    \n",
    "    def objective(params):\n",
    "        regressor.set_params(**params_fixed, **params)\n",
    "        regressor.fit(X_train, y_train)\n",
    "        \n",
    "        neg_mse = cross_val_score(regressor, X_train, y_train, cv=5, scoring=\"neg_mean_squared_error\").mean()\n",
    "        return {\"loss\": neg_mse, \"status\": STATUS_OK}\n",
    "    \n",
    "    if rstate is not None:\n",
    "        rstate = np.random.RandomState(rstate)\n",
    "        \n",
    "    trials = Trials()\n",
    "    best_params = fmin(objective, \n",
    "                      params_tuned, \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=num_eval, \n",
    "                      trials=trials,\n",
    "                      rstate=rstate)\n",
    "    \n",
    "    best_params = whole_to_int(best_params)\n",
    "    best_model = regressor.set_params(**params_fixed, **best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_elapse = time_end - time_start\n",
    "    print(\"Time elapsed: %0.5f s\" % time_elapse)\n",
    "    return trials, best_params, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolID</th>\n",
       "      <th>mol</th>\n",
       "      <th>Class</th>\n",
       "      <th>Model</th>\n",
       "      <th>IC50</th>\n",
       "      <th>EXP</th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>ExactMass</th>\n",
       "      <th>HeavyAtoms</th>\n",
       "      <th>Rings</th>\n",
       "      <th>...</th>\n",
       "      <th>MolecularVolume</th>\n",
       "      <th>RotatableBonds</th>\n",
       "      <th>HydrogenBondDonors</th>\n",
       "      <th>HydrogenBondAcceptors</th>\n",
       "      <th>SLogP</th>\n",
       "      <th>SMR</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>Fsp3Carbons</th>\n",
       "      <th>Sp3Carbons</th>\n",
       "      <th>MolecularComplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Triparanol</td>\n",
       "      <td>CC[NH](CCOc1ccc(cc1)[C@](c1ccc(cc1)C)(Cc1ccc(c...</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>7.05</td>\n",
       "      <td>-7.05</td>\n",
       "      <td>440.02</td>\n",
       "      <td>439.2278</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>428.55</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7.25</td>\n",
       "      <td>133.24</td>\n",
       "      <td>38.36</td>\n",
       "      <td>0.33</td>\n",
       "      <td>9</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tilorone</td>\n",
       "      <td>CC[NH](CCOc1ccc2c(c1)C(=O)c1c2ccc(c1)OCC[NH](C...</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>4.09</td>\n",
       "      <td>-7.38</td>\n",
       "      <td>414.58</td>\n",
       "      <td>414.2882</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>417.35</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7.15</td>\n",
       "      <td>130.24</td>\n",
       "      <td>53.33</td>\n",
       "      <td>0.48</td>\n",
       "      <td>12</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tideglusib</td>\n",
       "      <td>O=c1sn(c(=O)n1Cc1ccccc1)c1cccc2c1cccc2</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>1.55</td>\n",
       "      <td>-7.95</td>\n",
       "      <td>334.39</td>\n",
       "      <td>334.0776</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>275.91</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.06</td>\n",
       "      <td>97.44</td>\n",
       "      <td>44.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tetrandrine</td>\n",
       "      <td>COc1c(OC)cc2c3c1Oc1cc4c(cc1OC)CC[NH]([C@H]4Cc1...</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-7.56</td>\n",
       "      <td>626.78</td>\n",
       "      <td>626.3356</td>\n",
       "      <td>46</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>580.38</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>8.26</td>\n",
       "      <td>183.10</td>\n",
       "      <td>77.32</td>\n",
       "      <td>0.37</td>\n",
       "      <td>14</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shikonin</td>\n",
       "      <td>CC(=CC[C@H](C1=CC(=O)c2c(C1=O)c(O)ccc2O)O)C</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>15.75</td>\n",
       "      <td>-6.58</td>\n",
       "      <td>288.30</td>\n",
       "      <td>288.0998</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>275.21</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2.41</td>\n",
       "      <td>77.08</td>\n",
       "      <td>94.83</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         MolID                                                mol  Class  \\\n",
       "0   Triparanol  CC[NH](CCOc1ccc(cc1)[C@](c1ccc(cc1)C)(Cc1ccc(c...      1   \n",
       "1     Tilorone  CC[NH](CCOc1ccc2c(c1)C(=O)c1c2ccc(c1)OCC[NH](C...      1   \n",
       "2   Tideglusib             O=c1sn(c(=O)n1Cc1ccccc1)c1cccc2c1cccc2      1   \n",
       "3  Tetrandrine  COc1c(OC)cc2c3c1Oc1cc4c(cc1OC)CC[NH]([C@H]4Cc1...      1   \n",
       "4     Shikonin        CC(=CC[C@H](C1=CC(=O)c2c(C1=O)c(O)ccc2O)O)C      0   \n",
       "\n",
       "   Model   IC50   EXP  MolecularWeight  ExactMass  HeavyAtoms  Rings  ...  \\\n",
       "0  Train   7.05 -7.05           440.02   439.2278          31      3  ...   \n",
       "1  Train   4.09 -7.38           414.58   414.2882          30      3  ...   \n",
       "2  Train   1.55 -7.95           334.39   334.0776          24      4  ...   \n",
       "3  Train   3.00 -7.56           626.78   626.3356          46      8  ...   \n",
       "4  Train  15.75 -6.58           288.30   288.0998          21      2  ...   \n",
       "\n",
       "   MolecularVolume  RotatableBonds  HydrogenBondDonors  HydrogenBondAcceptors  \\\n",
       "0           428.55              10                   2                      3   \n",
       "1           417.35              12                   2                      5   \n",
       "2           275.91               3                   0                      4   \n",
       "3           580.38               4                   2                      8   \n",
       "4           275.21               3                   3                      5   \n",
       "\n",
       "   SLogP     SMR   TPSA  Fsp3Carbons  Sp3Carbons  MolecularComplexity  \n",
       "0   7.25  133.24  38.36         0.33           9                   56  \n",
       "1   7.15  130.24  53.33         0.48          12                   55  \n",
       "2   5.06   97.44  44.00         0.05           1                   62  \n",
       "3   8.26  183.10  77.32         0.37          14                   60  \n",
       "4   2.41   77.08  94.83         0.25           4                   38  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sars2_train = pd.read_csv(\"Sars2_train.csv\")\n",
    "Sars2_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 15), (52,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full_train = Sars2_train.drop([\"MolID\", \"mol\", \"Class\", \"Model\", \"IC50\", \"EXP\"], axis=\"columns\")\n",
    "y_full_train = Sars2_train[\"Class\"]\n",
    "\n",
    "X_full_train.shape, y_full_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>ExactMass</th>\n",
       "      <th>HeavyAtoms</th>\n",
       "      <th>Rings</th>\n",
       "      <th>AromaticRings</th>\n",
       "      <th>MolecularVolume</th>\n",
       "      <th>RotatableBonds</th>\n",
       "      <th>HydrogenBondDonors</th>\n",
       "      <th>HydrogenBondAcceptors</th>\n",
       "      <th>SLogP</th>\n",
       "      <th>SMR</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>Fsp3Carbons</th>\n",
       "      <th>Sp3Carbons</th>\n",
       "      <th>MolecularComplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>502.50</td>\n",
       "      <td>501.2076</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>462.20</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8.07</td>\n",
       "      <td>139.95</td>\n",
       "      <td>40.40</td>\n",
       "      <td>0.54</td>\n",
       "      <td>15</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>231.04</td>\n",
       "      <td>229.9579</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>158.79</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.98</td>\n",
       "      <td>47.75</td>\n",
       "      <td>46.53</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>302.71</td>\n",
       "      <td>302.0669</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>240.24</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.31</td>\n",
       "      <td>73.13</td>\n",
       "      <td>129.83</td>\n",
       "      <td>0.42</td>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>158.24</td>\n",
       "      <td>158.1419</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>175.11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.85</td>\n",
       "      <td>48.02</td>\n",
       "      <td>32.67</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>299.45</td>\n",
       "      <td>299.2249</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>315.81</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.31</td>\n",
       "      <td>90.85</td>\n",
       "      <td>43.09</td>\n",
       "      <td>0.65</td>\n",
       "      <td>13</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MolecularWeight  ExactMass  HeavyAtoms  Rings  AromaticRings  \\\n",
       "0           502.50   501.2076          34      5              2   \n",
       "1           231.04   229.9579          12      1              1   \n",
       "2           302.71   302.0669          20      2              2   \n",
       "3           158.24   158.1419          11      0              0   \n",
       "4           299.45   299.2249          22      3              1   \n",
       "\n",
       "   MolecularVolume  RotatableBonds  HydrogenBondDonors  HydrogenBondAcceptors  \\\n",
       "0           462.20               4                   1                      4   \n",
       "1           158.79               2                   1                      3   \n",
       "2           240.24               5                   6                      7   \n",
       "3           175.11               5                   0                      3   \n",
       "4           315.81               2                   1                      2   \n",
       "\n",
       "   SLogP     SMR    TPSA  Fsp3Carbons  Sp3Carbons  MolecularComplexity  \n",
       "0   8.07  139.95   40.40         0.54          15                   68  \n",
       "1   1.98   47.75   46.53         0.12           1                   37  \n",
       "2   0.31   73.13  129.83         0.42           5                   57  \n",
       "3   2.85   48.02   32.67         1.00           8                   37  \n",
       "4   4.31   90.85   43.09         0.65          13                   45  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_vitro_only = pd.read_csv(\"in-vitro-only.csv\")\n",
    "X_new = in_vitro_only[X_full_train.columns.to_list()]\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns droped due to collinearity: 5\n",
      "Number of columns droped due to collinearity: 5\n"
     ]
    }
   ],
   "source": [
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(X_full_train)\n",
    "X_full_train = remover.transform(X_full_train)\n",
    "X_new = remover.transform(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_full_train)\n",
    "X_full_train = scaler.transform(X_full_train)\n",
    "\n",
    "X_new = scaler.transform(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 54.42trial/s, best loss: -0.74]              \n",
      "Time elapsed: 0.92507 s\n",
      "best_params: {'C': 0.0007373893048654192}\n",
      "AUC of the train set: 0.75556\n",
      "AUC of the test set: 0.53571\n"
     ]
    }
   ],
   "source": [
    "# If we change random_state, we can get wildly different train and test AUC. \n",
    "# This is a symptom of too small dataset.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full_train, y_full_train, \n",
    "                                                    test_size=0.33, \n",
    "                                                    stratify=y_full_train, \n",
    "                                                    random_state=502)\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "params = {\"C\": hp.loguniform(\"C\", np.log(1e-8), np.log(1e8)),}\n",
    "\n",
    "params_fixed = {\"solver\": \"liblinear\"}\n",
    "\n",
    "num_eval = 50\n",
    "\n",
    "trials, best_params, best_model = hyperopt_cl(lr, params, X_train, y_train, num_eval, params_fixed=params_fixed)\n",
    "print(\"best_params:\", best_params)\n",
    "\n",
    "auc_train = roc_auc(best_model, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_test = roc_auc(best_model, X_test, y_test)\n",
    "print(\"AUC of the test set: %0.5f\" % auc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [15:39<00:00,  9.94s/trial, best loss: -0.74]\n",
      "Time elapsed: 941.15603 s\n",
      "best_params: {'max_depth': 9, 'max_features': 10, 'min_samples_leaf': 6, 'min_samples_split': 15}\n",
      "AUC of the train set: 0.88889\n",
      "AUC of the test set: 0.55357\n"
     ]
    }
   ],
   "source": [
    "# the problem is the same as above\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full_train, y_full_train, \n",
    "                                                    test_size=0.33, \n",
    "                                                    stratify=y_full_train, \n",
    "                                                    random_state=526)\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 2, 10, 1)),\n",
    "    \"min_samples_split\": scope.int(hp.quniform(\"min_samples_split\", 2, 20, 1)),\n",
    "    \"min_samples_leaf\": scope.int(hp.quniform(\"min_samples_leaf\", 2, 20, 1)), \n",
    "    \"max_features\": scope.int(hp.quniform(\"max_features\", 2, 10, 1)),\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    \"n_estimators\": 1000\n",
    "}\n",
    "\n",
    "\n",
    "num_eval = 100\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "trials, best_params, best_model = hyperopt_cl(rf, params, X_train, y_train, num_eval, params_fixed=params_fixed)\n",
    "print(\"best_params:\", best_params)\n",
    "\n",
    "auc_train = roc_auc(best_model, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_test = roc_auc(best_model, X_test, y_test)\n",
    "print(\"AUC of the test set: %0.5f\" % auc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:06<00:00,  3.05trial/s, best loss: -0.6333333333333333]\n",
      "Time elapsed: 66.44443 s\n",
      "best_params: {'colsample_bytree': 0.8447365518163015, 'gamma': 0.34488145829251876, 'learning_rate': 0.02652034980015437, 'max_depth': 6, 'min_child_weight': 2, 'reg_lambda': 0.8296259768148457, 'subsample': 0.5104958765374956}\n",
      "AUC of the train set: 0.75721\n",
      "AUC of the test set: 0.61538\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_full_train, y_full_train, \n",
    "                                                    test_size=0.33, \n",
    "                                                    stratify=y_full_train, \n",
    "                                                    random_state=432)\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 1, 6, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 16, 1)), \n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.2, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.2, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.0001), np.log(10000)),\n",
    "    #\"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.0001), np.log(100)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.001), np.log(0.5)),\n",
    "    \"gamma\": hp.uniform(\"gamma\", 0., 2.),\n",
    "}\n",
    "\n",
    "params_fixed_xgb = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"n_estimators\": 500\n",
    "}\n",
    "\n",
    "num_eval = 200\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "trials, best_params, best_model = hyperopt_cl(xgb, params, X_train, y_train, num_eval, params_fixed=params_fixed)\n",
    "print(\"best_params:\", best_params)\n",
    "\n",
    "auc_train = roc_auc(best_model, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_test = roc_auc(best_model, X_test, y_test)\n",
    "print(\"AUC of the test set: %0.5f\" % auc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.4 Densely connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_dl(estimator, X_eval, y_eval):\n",
    "    \"\"\"\n",
    "    :param estimator: sklearn estimator that have predict_proba() method\n",
    "    :param X_eval: test features\n",
    "    :param y_eval: test target\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    proba = estimator.predict(X_eval)\n",
    "    return roc_auc_score(y_eval, proba[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dense_model(input_shape, \n",
    "                     n_hiddens=2, \n",
    "                     n_units_per_hidden=100,\n",
    "                     activation=\"elu\",\n",
    "                     kernel_initializer=\"he_normal\",\n",
    "                     l2_regularizer=None,\n",
    "                     batch_normalization=True,\n",
    "                     loss=\"binary_crossentropy\",\n",
    "                     optimizer=None,\n",
    "                     learning_rate=1e-3,\n",
    "                     decay=0.,\n",
    "                     dropout_rate=0.,\n",
    "                     metrics=None):\n",
    "    if optimizer is None:\n",
    "        optimizer = keras.optimizers.Adam\n",
    "    optimizer = optimizer(lr=learning_rate, decay=decay)\n",
    "    print(\"Use optimizer:\", optimizer)\n",
    "    \n",
    "    if l2_regularizer is None:\n",
    "        kernel_regularizer = None\n",
    "    else:\n",
    "        print(\"Use l2 regularizer:\", l2_regularizer)\n",
    "        kernel_regularizer = keras.regularizers.l2(l2_regularizer)\n",
    "    \n",
    "    if batch_normalization and activation == \"selu\":\n",
    "        raise ValueError(\"Batch normalization should not be used together with the activation selu\")\n",
    "    \n",
    "    if batch_normalization:\n",
    "        print(\"Use batch normalization\")\n",
    "    \n",
    "    if metrics is None:\n",
    "        auc = tf.keras.metrics.AUC()\n",
    "        metrics = [auc]\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    #\n",
    "    for i in range(n_hiddens):\n",
    "        if i == 0:\n",
    "            model.add(keras.layers.Dense(n_units_per_hidden, \n",
    "                                         activation=activation,\n",
    "                                         kernel_initializer=kernel_initializer,\n",
    "                                         kernel_regularizer=kernel_regularizer,\n",
    "                                         input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(keras.layers.Dense(n_units_per_hidden,\n",
    "                                         kernel_initializer=kernel_initializer,\n",
    "                                         kernel_regularizer=kernel_regularizer,\n",
    "                                         activation=activation))\n",
    "        if batch_normalization:\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    if n_hiddens > 0:\n",
    "        model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    else:\n",
    "        model.add(keras.layers.Dense(1, activation=\"sigmoid\", input_shape=input_shape))\n",
    "    \n",
    "    if dropout_rate > 0:\n",
    "        if activation != \"selu\":\n",
    "            model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "        else:\n",
    "            model.add(keras.layers.AlphaDropout(rate=dropout_rate))\n",
    "        \n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "def train(model, X_train, y_train, validation_data, epochs=100, batch_size=32):\n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, \n",
    "                                                  monitor=\"val_auc\", \n",
    "                                                  mode=\"max\", \n",
    "                                                  restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, batch_size=batch_size,\n",
    "                    validation_data=validation_data,\n",
    "                    callbacks=[early_stopping_cb],\n",
    "                    use_multiprocessing=True)\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use optimizer: <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x193a5b60b8>\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 0.7962 - auc: 0.4600 - val_loss: 0.6861 - val_auc: 0.7232\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.7948 - auc: 0.4600 - val_loss: 0.6854 - val_auc: 0.7321\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.7937 - auc: 0.4600 - val_loss: 0.6848 - val_auc: 0.7321\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.7927 - auc: 0.4600 - val_loss: 0.6843 - val_auc: 0.7411\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.7918 - auc: 0.4600 - val_loss: 0.6838 - val_auc: 0.7411\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.7908 - auc: 0.4578 - val_loss: 0.6833 - val_auc: 0.7411\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.7900 - auc: 0.4578 - val_loss: 0.6827 - val_auc: 0.7411\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.7891 - auc: 0.4578 - val_loss: 0.6822 - val_auc: 0.7411\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.7883 - auc: 0.4578 - val_loss: 0.6817 - val_auc: 0.7411\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.7875 - auc: 0.4578 - val_loss: 0.6813 - val_auc: 0.7500\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.7866 - auc: 0.4578 - val_loss: 0.6807 - val_auc: 0.7589\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.7858 - auc: 0.4578 - val_loss: 0.6800 - val_auc: 0.7589\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.7849 - auc: 0.4578 - val_loss: 0.6794 - val_auc: 0.7679\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.7842 - auc: 0.4578 - val_loss: 0.6788 - val_auc: 0.7679\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.7834 - auc: 0.4578 - val_loss: 0.6782 - val_auc: 0.7679\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.7827 - auc: 0.4578 - val_loss: 0.6775 - val_auc: 0.7679\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.7820 - auc: 0.4556 - val_loss: 0.6769 - val_auc: 0.7679\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.7812 - auc: 0.4556 - val_loss: 0.6763 - val_auc: 0.7679\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.7804 - auc: 0.4556 - val_loss: 0.6756 - val_auc: 0.7679\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.7795 - auc: 0.4556 - val_loss: 0.6749 - val_auc: 0.7679\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.7785 - auc: 0.4556 - val_loss: 0.6741 - val_auc: 0.7679\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.7775 - auc: 0.4556 - val_loss: 0.6732 - val_auc: 0.7679\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.7765 - auc: 0.4556 - val_loss: 0.6723 - val_auc: 0.7679\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_full_train, y_full_train, \n",
    "                                                    test_size=0.33, \n",
    "                                                    stratify=y_full_train, \n",
    "                                                    random_state=526)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "\n",
    "input_shape = X_train.shape\n",
    "        \n",
    "model = make_dense_model(input_shape[-1:], \n",
    "                          n_hiddens=1, \n",
    "                          n_units_per_hidden=20,\n",
    "                          activation=\"elu\",\n",
    "                          kernel_initializer=\"he_normal\",\n",
    "                          l2_regularizer=None,\n",
    "                          batch_normalization=False,\n",
    "                          learning_rate=1e-4)\n",
    "\n",
    "\n",
    "history, model = train(model, X_train, y_train, (X_test, y_test), epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x193af34e80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "ax[0].plot(history.history[\"loss\"], label=\"loss\")\n",
    "ax[0].plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(history.history[\"auc\"], label=\"auc\")\n",
    "ax[1].plot(history.history[\"val_auc\"], label=\"val_auc\")\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of the train set: 0.45333\n",
      "AUC of the test set: 0.76786\n"
     ]
    }
   ],
   "source": [
    "auc_train = roc_auc_dl(model, X_train, y_train)\n",
    "print(\"AUC of the train set: %0.5f\" % auc_train)\n",
    "\n",
    "auc_test = roc_auc_dl(model, X_test, y_test)\n",
    "print(\"AUC of the test set: %0.5f\" % auc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolID</th>\n",
       "      <th>mol</th>\n",
       "      <th>Class</th>\n",
       "      <th>Model</th>\n",
       "      <th>IC50</th>\n",
       "      <th>EXP</th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>ExactMass</th>\n",
       "      <th>HeavyAtoms</th>\n",
       "      <th>Rings</th>\n",
       "      <th>...</th>\n",
       "      <th>MolecularVolume</th>\n",
       "      <th>RotatableBonds</th>\n",
       "      <th>HydrogenBondDonors</th>\n",
       "      <th>HydrogenBondAcceptors</th>\n",
       "      <th>SLogP</th>\n",
       "      <th>SMR</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>Fsp3Carbons</th>\n",
       "      <th>Sp3Carbons</th>\n",
       "      <th>MolecularComplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Triparanol</td>\n",
       "      <td>CC[NH](CCOc1ccc(cc1)[C@](c1ccc(cc1)C)(Cc1ccc(c...</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>7.05</td>\n",
       "      <td>-7.05</td>\n",
       "      <td>440.02</td>\n",
       "      <td>439.2278</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>428.55</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7.25</td>\n",
       "      <td>133.24</td>\n",
       "      <td>38.36</td>\n",
       "      <td>0.33</td>\n",
       "      <td>9</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tilorone</td>\n",
       "      <td>CC[NH](CCOc1ccc2c(c1)C(=O)c1c2ccc(c1)OCC[NH](C...</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>4.09</td>\n",
       "      <td>-7.38</td>\n",
       "      <td>414.58</td>\n",
       "      <td>414.2882</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>417.35</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7.15</td>\n",
       "      <td>130.24</td>\n",
       "      <td>53.33</td>\n",
       "      <td>0.48</td>\n",
       "      <td>12</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tideglusib</td>\n",
       "      <td>O=c1sn(c(=O)n1Cc1ccccc1)c1cccc2c1cccc2</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>1.55</td>\n",
       "      <td>-7.95</td>\n",
       "      <td>334.39</td>\n",
       "      <td>334.0776</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>275.91</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.06</td>\n",
       "      <td>97.44</td>\n",
       "      <td>44.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tetrandrine</td>\n",
       "      <td>COc1c(OC)cc2c3c1Oc1cc4c(cc1OC)CC[NH]([C@H]4Cc1...</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-7.56</td>\n",
       "      <td>626.78</td>\n",
       "      <td>626.3356</td>\n",
       "      <td>46</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>580.38</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>8.26</td>\n",
       "      <td>183.10</td>\n",
       "      <td>77.32</td>\n",
       "      <td>0.37</td>\n",
       "      <td>14</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shikonin</td>\n",
       "      <td>CC(=CC[C@H](C1=CC(=O)c2c(C1=O)c(O)ccc2O)O)C</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>15.75</td>\n",
       "      <td>-6.58</td>\n",
       "      <td>288.30</td>\n",
       "      <td>288.0998</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>275.21</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2.41</td>\n",
       "      <td>77.08</td>\n",
       "      <td>94.83</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         MolID                                                mol  Class  \\\n",
       "0   Triparanol  CC[NH](CCOc1ccc(cc1)[C@](c1ccc(cc1)C)(Cc1ccc(c...      1   \n",
       "1     Tilorone  CC[NH](CCOc1ccc2c(c1)C(=O)c1c2ccc(c1)OCC[NH](C...      1   \n",
       "2   Tideglusib             O=c1sn(c(=O)n1Cc1ccccc1)c1cccc2c1cccc2      1   \n",
       "3  Tetrandrine  COc1c(OC)cc2c3c1Oc1cc4c(cc1OC)CC[NH]([C@H]4Cc1...      1   \n",
       "4     Shikonin        CC(=CC[C@H](C1=CC(=O)c2c(C1=O)c(O)ccc2O)O)C      0   \n",
       "\n",
       "   Model   IC50   EXP  MolecularWeight  ExactMass  HeavyAtoms  Rings  ...  \\\n",
       "0  Train   7.05 -7.05           440.02   439.2278          31      3  ...   \n",
       "1  Train   4.09 -7.38           414.58   414.2882          30      3  ...   \n",
       "2  Train   1.55 -7.95           334.39   334.0776          24      4  ...   \n",
       "3  Train   3.00 -7.56           626.78   626.3356          46      8  ...   \n",
       "4  Train  15.75 -6.58           288.30   288.0998          21      2  ...   \n",
       "\n",
       "   MolecularVolume  RotatableBonds  HydrogenBondDonors  HydrogenBondAcceptors  \\\n",
       "0           428.55              10                   2                      3   \n",
       "1           417.35              12                   2                      5   \n",
       "2           275.91               3                   0                      4   \n",
       "3           580.38               4                   2                      8   \n",
       "4           275.21               3                   3                      5   \n",
       "\n",
       "   SLogP     SMR   TPSA  Fsp3Carbons  Sp3Carbons  MolecularComplexity  \n",
       "0   7.25  133.24  38.36         0.33           9                   56  \n",
       "1   7.15  130.24  53.33         0.48          12                   55  \n",
       "2   5.06   97.44  44.00         0.05           1                   62  \n",
       "3   8.26  183.10  77.32         0.37          14                   60  \n",
       "4   2.41   77.08  94.83         0.25           4                   38  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sars2_train = pd.read_csv(\"Sars2_train.csv\")\n",
    "Sars2_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52, 15), (52,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full_train = Sars2_train.drop([\"MolID\", \"mol\", \"Class\", \"Model\", \"IC50\", \"EXP\"], axis=\"columns\")\n",
    "y_full_train = Sars2_train[\"EXP\"]\n",
    "\n",
    "X_full_train.shape, y_full_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.553846153846154, 0.999480558906463)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_full_train.mean(), y_full_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>ExactMass</th>\n",
       "      <th>HeavyAtoms</th>\n",
       "      <th>Rings</th>\n",
       "      <th>AromaticRings</th>\n",
       "      <th>MolecularVolume</th>\n",
       "      <th>RotatableBonds</th>\n",
       "      <th>HydrogenBondDonors</th>\n",
       "      <th>HydrogenBondAcceptors</th>\n",
       "      <th>SLogP</th>\n",
       "      <th>SMR</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>Fsp3Carbons</th>\n",
       "      <th>Sp3Carbons</th>\n",
       "      <th>MolecularComplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>502.50</td>\n",
       "      <td>501.2076</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>462.20</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8.07</td>\n",
       "      <td>139.95</td>\n",
       "      <td>40.40</td>\n",
       "      <td>0.54</td>\n",
       "      <td>15</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>231.04</td>\n",
       "      <td>229.9579</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>158.79</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.98</td>\n",
       "      <td>47.75</td>\n",
       "      <td>46.53</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>302.71</td>\n",
       "      <td>302.0669</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>240.24</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.31</td>\n",
       "      <td>73.13</td>\n",
       "      <td>129.83</td>\n",
       "      <td>0.42</td>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>158.24</td>\n",
       "      <td>158.1419</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>175.11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.85</td>\n",
       "      <td>48.02</td>\n",
       "      <td>32.67</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>299.45</td>\n",
       "      <td>299.2249</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>315.81</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.31</td>\n",
       "      <td>90.85</td>\n",
       "      <td>43.09</td>\n",
       "      <td>0.65</td>\n",
       "      <td>13</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MolecularWeight  ExactMass  HeavyAtoms  Rings  AromaticRings  \\\n",
       "0           502.50   501.2076          34      5              2   \n",
       "1           231.04   229.9579          12      1              1   \n",
       "2           302.71   302.0669          20      2              2   \n",
       "3           158.24   158.1419          11      0              0   \n",
       "4           299.45   299.2249          22      3              1   \n",
       "\n",
       "   MolecularVolume  RotatableBonds  HydrogenBondDonors  HydrogenBondAcceptors  \\\n",
       "0           462.20               4                   1                      4   \n",
       "1           158.79               2                   1                      3   \n",
       "2           240.24               5                   6                      7   \n",
       "3           175.11               5                   0                      3   \n",
       "4           315.81               2                   1                      2   \n",
       "\n",
       "   SLogP     SMR    TPSA  Fsp3Carbons  Sp3Carbons  MolecularComplexity  \n",
       "0   8.07  139.95   40.40         0.54          15                   68  \n",
       "1   1.98   47.75   46.53         0.12           1                   37  \n",
       "2   0.31   73.13  129.83         0.42           5                   57  \n",
       "3   2.85   48.02   32.67         1.00           8                   37  \n",
       "4   4.31   90.85   43.09         0.65          13                   45  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_vitro_only = pd.read_csv(\"in-vitro-only.csv\")\n",
    "X_new = in_vitro_only[X_full_train.columns.to_list()]\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns droped due to collinearity: 5\n",
      "Number of columns droped due to collinearity: 5\n"
     ]
    }
   ],
   "source": [
    "remover = CollinearColumnRemover(0.95)\n",
    "remover.fit(X_full_train)\n",
    "X_full_train = remover.transform(X_full_train)\n",
    "X_new = remover.transform(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_full_train)\n",
    "X_full_train = scaler.transform(X_full_train)\n",
    "\n",
    "X_new = scaler.transform(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of the train set: 0.59210\n",
      "R2 of the train set: 0.28716\n",
      "RMSE of the test set: 1.53347\n",
      "R2 of the test set: -0.56686\n"
     ]
    }
   ],
   "source": [
    "# the result is really bad\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full_train, y_full_train, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=530)\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "rmse_train = rmse(lr, X_train, y_train)\n",
    "print(\"RMSE of the train set: %0.5f\" % rmse_train)\n",
    "r2_train = r2(lr, X_train, y_train)\n",
    "print(\"R2 of the train set: %0.5f\" % r2_train)\n",
    "\n",
    "rmse_test = rmse(lr, X_test, y_test)\n",
    "print(\"RMSE of the test set: %0.5f\" % rmse_test)\n",
    "r2_test = r2(lr, X_test, y_test)\n",
    "print(\"R2 of the test set: %0.5f\" % r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 69.16trial/s, best loss: -1.3742849653317974]\n",
      "Time elapsed: 0.72911 s\n",
      "best_params: {'alpha': 0.01049417522683663}\n",
      "RMSE of the train set: 0.23053\n",
      "R2 of the train set: 0.26989\n",
      "RMSE of the test set: 6.54972\n",
      "R2 of the test set: -0.63783\n"
     ]
    }
   ],
   "source": [
    "# the result is really bad\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full_train, y_full_train, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=402)\n",
    "\n",
    "\n",
    "ridge = Ridge()\n",
    "\n",
    "params = {\"alpha\": hp.loguniform(\"alpha\", np.log(1e-2), np.log(1e5)),}\n",
    "\n",
    "num_eval = 50\n",
    "\n",
    "trials, best_params, best_model = hyperopt_reg(ridge, params, X_train, y_train, num_eval)\n",
    "print(\"best_params:\", best_params)\n",
    "\n",
    "\n",
    "rmse_train = rmse(best_model, X_train, y_train)\n",
    "print(\"RMSE of the train set: %0.5f\" % rmse_train)\n",
    "r2_train = r2(best_model, X_train, y_train)\n",
    "print(\"R2 of the train set: %0.5f\" % r2_train)\n",
    "\n",
    "rmse_test = rmse(best_model, X_test, y_test)\n",
    "print(\"RMSE of the test set: %0.5f\" % rmse_test)\n",
    "r2_test = r2(best_model, X_test, y_test)\n",
    "print(\"R2 of the test set: %0.5f\" % r2_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
